---
alwaysApply: true
---

# üîß Backend Development Flow - RAG Learning Engine

## Core Focus
Simple backend for uploading documents/URLs, managing document library, and processing through RAG pipeline.

---

## Backend Requirements

### What We Need
- **Upload**: Single document or URL at a time
- **Library**: Always show user's documents on frontend
- **No Sessions**: Stateless - no user management needed
- **RAG Processing**: Extract ‚Üí Chunk ‚Üí Embed ‚Üí Store

### What We Don't Need
- User authentication/sessions
- Complex file management
- Batch operations
- Advanced security

---

## Backend Architecture

```
Frontend (Streamlit) ‚Üê‚Üí FastAPI Backend ‚Üê‚Üí RAG Pipeline
                              ‚Üì
                        SQLite Database
                              ‚Üì
                        Qdrant Vector DB
```

---

## Database Schema (SQLite)

```sql
-- Documents table
CREATE TABLE documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    filename TEXT NOT NULL,           -- For files: original filename, For URLs: extracted title
    original_name TEXT,               -- Original filename for uploaded files
    file_path TEXT,                   -- Local file path (NULL for URLs)
    source_url TEXT,                  -- URL source (NULL for uploaded files)
    content_type TEXT,                -- file/url to distinguish source type
    file_type TEXT,                   -- pdf, txt, md, doc, html, etc.
    file_size INTEGER,                -- File size (NULL for URLs)
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processing_status TEXT DEFAULT 'pending',  -- pending, processing, completed, failed
    extracted_text TEXT              -- Content extracted from file or URL
);

-- Chunks table for RAG
CREATE TABLE chunks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER,
    chunk_text TEXT NOT NULL,
    chunk_index INTEGER,
    chunk_type TEXT,  -- text, table, image
    embedding_id TEXT,  -- Reference to Qdrant vector
    FOREIGN KEY (document_id) REFERENCES documents(id)
);
```

---

## API Endpoints

### 1. Upload Document
```http
POST /upload
Content-Type: multipart/form-data

Body: file (PDF, TXT, MD)
Response: {
    "document_id": 123,
    "filename": "lecture_notes.pdf", 
    "status": "uploaded",
    "processing_status": "pending"
}
```

### 2. Extract from URL (LangChain WebLoader)
```http
POST /extract-url
Content-Type: application/json

Body: {
    "url": "https://arxiv.org/pdf/1234.5678.pdf"
}
Response: {
    "document_id": 124,
    "title": "Attention Is All You Need",
    "url": "https://arxiv.org/pdf/1234.5678.pdf",
    "status": "extracted", 
    "processing_status": "pending"
}
```

### 3. Get Document Library
```http
GET /documents
Response: {
    "documents": [
        {
            "id": 123,
            "filename": "lecture_notes.pdf",
            "file_type": "pdf",
            "upload_date": "2024-01-15T10:30:00",
            "processing_status": "completed",
            "chunk_count": 45
        }
    ]
}
```

### 4. Delete Document
```http
DELETE /documents/{id}
Response: {
    "message": "Document deleted successfully"
}
```

### 5. Query RAG
```http
POST /query
Content-Type: application/json

Body: {
    "question": "What is machine learning?"
}
Response: {
    "answer": "Machine learning is...",
    "sources": [
        {
            "document_id": 123,
            "chunk_id": 456,
            "filename": "lecture_notes.pdf",
            "chunk_text": "ML definition...",
            "relevance_score": 0.95
        }
    ]
}
```

---

## Backend Processing Flow

### 1. Upload Flow (Files vs URLs)

**File Upload Flow:**
```
File Upload ‚Üí Save to /data/uploads ‚Üí Store Metadata ‚Üí Trigger RAG Processing
```

**URL Extraction Flow:**
```
URL Input ‚Üí LangChain WebLoader ‚Üí Extract Content ‚Üí Store Metadata ‚Üí Trigger RAG Processing
```

**File Upload Steps:**
1. Receive file (PDF, DOC, TXT, MD) from frontend
2. Save file to `/data/uploads/` directory
3. Insert record in `documents` table with file_path
4. Start background RAG processing
5. Return document_id to frontend

**URL Extraction Steps:**
1. Receive URL from frontend
2. Use LangChain WebLoader to extract content directly
3. Insert record in `documents` table with source_url (no file_path)
4. Start background RAG processing with extracted content
5. Return document_id to frontend

### 2. RAG Processing Pipeline
```
Document ‚Üí Extract Text ‚Üí Chunk ‚Üí Generate Embeddings ‚Üí Store in Vector DB
```

**Steps:**
1. Extract text from PDF/document
2. Clean and process text
3. Split into semantic chunks
4. Generate embeddings using sentence-transformers
5. Store chunks in SQLite + embeddings in Qdrant
6. Update `processing_status` to 'completed'

### 3. Query Flow
```
Question ‚Üí Retrieve Chunks ‚Üí Rerank ‚Üí Generate Response ‚Üí Format with Citations
```

**Steps:**
1. Receive question from frontend
2. Generate query embedding
3. Search similar chunks in Qdrant
4. Rerank results using cross-encoder
5. Generate answer using LLM with context
6. Format response with source citations
7. Return to frontend

---

## Backend File Structure

```
src/backend/
‚îú‚îÄ‚îÄ main.py              # FastAPI app entry point
‚îú‚îÄ‚îÄ models.py            # SQLite models & Pydantic schemas
‚îú‚îÄ‚îÄ routes.py            # API endpoint handlers
‚îú‚îÄ‚îÄ config.py            # Configuration settings
‚îî‚îÄ‚îÄ services/
    ‚îú‚îÄ‚îÄ upload_service.py    # Handle file/URL uploads
    ‚îú‚îÄ‚îÄ rag_service.py       # RAG pipeline orchestration
    ‚îî‚îÄ‚îÄ document_service.py  # Document CRUD operations
```

---

## Key Components

### 1. FastAPI App (`main.py`)
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from routes import router

app = FastAPI(title="AI Learning Engine API")

# Enable CORS for Streamlit
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(router)
```

### 2. Database Models (`models.py`)
```python
from sqlalchemy import Column, Integer, String, Text, DateTime
from pydantic import BaseModel

# SQLAlchemy ORM Models
class Document(Base):
    __tablename__ = "documents"
    id = Column(Integer, primary_key=True)
    filename = Column(String, nullable=False)
    # ... other fields

# Pydantic Schemas
class DocumentResponse(BaseModel):
    id: int
    filename: str
    processing_status: str
    upload_date: datetime
```

### 3. Upload Service (`upload_service.py`)
```python
from langchain.document_loaders import WebBaseLoader, PyPDFLoader

async def upload_file(file: UploadFile):
    # Save file to /data/uploads/
    # Insert metadata to database with file_path
    # Trigger RAG processing
    # Return document info

async def extract_url_content(url: str):
    # Use LangChain WebLoader to extract content
    # loader = WebBaseLoader(url) or appropriate loader
    # content = loader.load()
    # Insert metadata to database with source_url (no file_path)
    # Trigger RAG processing with extracted content
    # Return document info
```

### 4. RAG Service (`rag_service.py`)
```python
async def process_document(document_id: int):
    # Extract text from document
    # Create chunks
    # Generate embeddings
    # Store in vector database
    # Update processing status

async def query_rag(question: str):
    # Retrieve relevant chunks
    # Rerank results
    # Generate answer with LLM
    # Format with citations
    # Return response
```

---

## Development Priorities

### Phase 1: Basic API (Week 1)
- [ ] Setup FastAPI app with CORS
- [ ] Create SQLite database and models
- [ ] Implement file upload endpoint
- [ ] Implement document library endpoint
- [ ] Test with Streamlit frontend

### Phase 2: RAG Pipeline (Week 2)
- [ ] Add text extraction (PyMuPDF)
- [ ] Implement document chunking
- [ ] Setup Qdrant vector database
- [ ] Create embedding pipeline
- [ ] Add query endpoint

### Phase 3: Advanced Features (Week 3)
- [ ] Add LangChain WebLoader for URL content extraction
- [ ] Implement reranking with cross-encoder
- [ ] Add citation formatting
- [ ] Optimize query response time

---

## Quick Start Commands

```bash
# Start FastAPI backend
cd src/backend
uvicorn main:app --reload --port 8000

# Start Qdrant vector database
docker run -p 6333:6333 qdrant/qdrant

# Test API endpoints
curl -X GET http://localhost:8000/documents
curl -X POST http://localhost:8000/upload -F "file=@test.pdf"
curl -X POST http://localhost:8000/extract-url -H "Content-Type: application/json" -d '{"url":"https://example.com/paper.pdf"}'
```

---

## Testing Strategy

### Manual Testing
- [ ] Upload PDF/TXT/MD files and verify storage in /data/uploads
- [ ] Test URL content extraction with LangChain WebLoader
- [ ] Check documents appear in library (both files and URLs)
- [ ] Query uploaded documents and extracted URL content
- [ ] Verify citations are accurate for both sources

### API Testing
- [ ] Test all endpoints with Postman/curl
- [ ] Verify error handling
- [ ] Test with different file types
- [ ] Validate response formats

---

**Focus**: Keep it simple, get RAG pipeline working end-to-end, then optimize! üéØ
description:
globs:
alwaysApply: false
---
