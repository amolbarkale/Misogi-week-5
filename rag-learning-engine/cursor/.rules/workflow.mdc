---
alwaysApply: true
---

# Assignment — **AI Learning Engine**  
*Turn your notes, slides, flow‑charts, and textbooks into an always‑on AI tutor.*
---
## Objective  
Create a **personal AI learning platform** that digests **all** of your AI‑related study materials—PDF notes, markdown cheat‑sheets, hand‑drawn architecture diagrams, lecture slide decks, flow‑charts, research papers—and turns them into an interactive, citation‑rich tutor that helps you explore every concept with "infinite potential."
---
## Core Requirements  
1. **Document Upload & Processing**  
   - Ingest lecture notes, slide decks, textbooks, research papers, and diagrams via file upload or URL.  
2. **Query Decomposition**  
   - Break complex learning objectives into prerequisite concepts and sub‑topics.  
3. **Contextual Compression**  
   - Filter relevant educational insights from extensive study materials.  
4. **Hybrid RAG**  
   - Combine dense + sparse retrieval for comprehensive academic understanding.  
5. **Advanced Reranking**  
   - Employ a multi‑model ensemble—including cross‑encoder models—for pedagogically relevant results.  
6. **Citation‑based Responses**  
   - Provide source tracking for all explanations and references.  
---
## Technical Requirements  
| Layer | Tech Stack / Tools |
| ----- | ------------------ |
| **Backend** | FastAPI with SQLite database |
| **Frontend** | Streamlit with built-in components |
| **Document Storage** | SQLite for metadata &nbsp;•&nbsp; Local file system (/data/uploads) for files &nbsp;•&nbsp; **Vector DB** (e.g., Qdrant, Pinecone) for document embeddings |
| **Tool Calling** | External APIs (market data, research repositories) as needed |
| **LLMs** | Multiple models optimized for distinct educational analysis tasks |
| **Charts & Tables** | Streamlit native components for tables and visualizations |
---
## Use‑Cases (Education‑Centric)
1. **"Upload my machine learning textbook and ask: 'Explain gradient descent with examples from my materials.'"**
2. **"Provide a URL to a research paper and ask: 'What are the key concepts I need to understand before reading this paper?'"**
3. **"Compare different optimization algorithms based on my uploaded lecture notes and research papers."**
---
## Deliverables
- **Source‑code repository** with clear setup instructions  
- **Streamlit demo app** showcasing end‑to‑end RAG workflow  
- **Technical report** summarizing architecture, design choices, and RAG implementation  
---
> **Tip:** Focus on modular design—keep ingestion, retrieval, reranking, and generation loosely coupled so each piece can evolve independently.

---

# AI Learning Engine — Enhanced Strategic Workflow  

## Strategic Architecture Overview

### Core Design Principles
- **Microservice Architecture**: Decouple document processing, retrieval, and generation services
- **Event-Driven Processing**: Use message queues for async document processing and indexing
- **Multi-Modal Intelligence**: Handle text, diagrams, charts, and mathematical equations
- **Adaptive Learning**: System learns from user interactions to improve recommendations

### Advanced RAG Strategy
```
Query → Decomposition → Multi-Route Retrieval → Ensemble Reranking → Generation
    ↓           ↓              ↓                    ↓              ↓
Intent      Sub-queries    Hybrid Results      Pedagogical      Contextual
Analysis    Generation     Fusion              Scoring         Citations
```

---

## Phase 0: Preparation & Project Setup  
**Goal:** Establish your environment and repo scaffold before writing any feature code.

### Enhanced Setup
1. **Create repo scaffold** with simple service structure:
   ```
   ai-learning-engine/
   ├── services/
   │   ├── ingestion/     # Document processing service
   │   ├── retrieval/     # RAG engine service  
   │   └── generation/    # LLM orchestration service
   ├── shared/            # Common utilities, schemas
   │   ├── database/      # SQLite models and migrations
   │   └── config.py      # Settings and environment variables
   ├── frontend/          # Streamlit application
   ├── infrastructure/    # Docker configs
   └── data/
       ├── uploads/       # Local document storage
       └── documents.db   # SQLite database
   ```

2. **Setup development environment**:
   - Use `poetry` for Python dependency management
   - Docker Compose with services: Qdrant
   - SQLite database at `/data/documents.db` for metadata
   - Local file system mounted at `/data/uploads` for document storage
   - Pre-commit hooks for code quality

3. **Configure environment variables** with validation:
   ```python
   # Use Pydantic for environment validation
   class Settings(BaseSettings):
       openai_api_key: str
       qdrant_url: str
       database_url: str = "sqlite:///data/documents.db"
       uploads_directory: str = "/data/uploads"
   ```

**Deliverable:** Fully containerized development environment

---

## Phase 1: Document Ingestion Service  
**Goal:** Build a simple, effective document processing pipeline with multi-modal support.

### Core Features
- **Multi-Modal Extraction**: 
  - OCR for scanned documents (Tesseract + EasyOCR)
  - Diagram extraction from images using computer vision
  - LaTeX/equation parsing for mathematical content
- **Document Fingerprinting**: Prevent duplicate ingestion using content hashing
- **Smart Chunking**: Context-aware chunking based on document structure

### API Endpoints
- `POST /ingest/upload` → upload files to /data/uploads and store metadata in SQLite
- `POST /ingest/url` → download documents from URLs and store in SQLite + filesystem
- `GET /documents` → retrieve document list from SQLite with metadata
- `GET /documents/{id}` → get specific document details from SQLite
- `DELETE /documents/{id}` → remove document from SQLite and filesystem
- `GET /documents/{id}/chunks` → retrieve document chunks for RAG queries

### Document Processing Pipeline
```python
Document/URL → Local Storage → Content Extraction → Structure Analysis → Chunk Generation → Embedding → Vector Store
      ↓            ↓               ↓                    ↓               ↓              ↓           ↓
  Upload/       /data/uploads    Multi-modal        Hierarchical     Semantic        Dense        Indexed
  Download      + SQLite DB      Processing         Parsing          Boundaries     Vectors      & Ready
                Metadata
```

### SQLite Database Schema
```sql
-- Documents table for metadata
CREATE TABLE documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    filename TEXT NOT NULL,
    original_filename TEXT,
    file_path TEXT NOT NULL,
    file_type TEXT,
    file_size INTEGER,
    source_url TEXT,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processing_status TEXT DEFAULT 'pending',
    extracted_text TEXT,
    metadata JSON
);

-- Chunks table for RAG
CREATE TABLE chunks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER,
    chunk_text TEXT NOT NULL,
    chunk_index INTEGER,
    semantic_level INTEGER,
    chunk_type TEXT,
    embedding_id TEXT,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

-- Processing logs
CREATE TABLE processing_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER,
    stage TEXT,
    status TEXT,
    error_message TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);
```

### FastAPI + SQLite Integration Example
```python
from fastapi import FastAPI, UploadFile, File
from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# Database models
Base = declarative_base()

class Document(Base):
    __tablename__ = "documents"
    id = Column(Integer, primary_key=True)
    filename = Column(String, nullable=False)
    file_path = Column(String, nullable=False)
    file_type = Column(String)
    source_url = Column(String)
    processing_status = Column(String, default="pending")
    extracted_text = Column(Text)

# FastAPI endpoints
@app.post("/ingest/upload")
async def upload_document(file: UploadFile = File(...)):
    # Save file to /data/uploads
    file_path = f"/data/uploads/{file.filename}"
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    # Store metadata in SQLite
    doc = Document(
        filename=file.filename,
        file_path=file_path,
        file_type=file.content_type
    )
    db.add(doc)
    db.commit()
    
    return {"document_id": doc.id, "status": "uploaded"}
```

**Deliverable:** Simple, effective ingestion service with comprehensive document support and SQLite integration

---

## Phase 2: Simple Streamlit Interface  
**Goal:** Create a clean, intuitive Streamlit UI for document upload and question asking.

### Core UI Components
- **Document Upload Interface**: Streamlit file uploader and text input for URLs
- **Query Interface**: Simple text input with submit button
- **Response Display**: Formatted text with expandable sections for citations
- **Document List**: Streamlit dataframe or list view of uploaded documents

### Streamlit App Structure
```
frontend/
├── main.py                 # Main Streamlit app
├── components/
│   ├── upload_handler.py   # File upload and URL processing
│   ├── query_interface.py  # Query input and submission
│   ├── response_display.py # Answer formatting with citations
│   └── document_manager.py # Document list and management
└── utils/
    └── api_client.py       # Simple API calls to backend
```

### Streamlit Features
- **Built-in Components**: File uploader, text input, dataframes, expandable sections
- **Session State**: Simple session state management for uploaded documents and chat history
- **Direct API Integration**: Simple requests to FastAPI backend

### SQLite Benefits for RAG System
- **Structured Queries**: SQL-based filtering of documents and chunks by metadata
- **Relationship Management**: Foreign keys between documents, chunks, and processing logs  
- **Full-Text Search**: Built-in FTS for quick text searching within document content
- **Transaction Support**: ACID compliance for data integrity during processing
- **Simple Deployment**: Single file database with no server setup required
- **Performance**: Fast queries for document metadata and chunk retrieval

**Deliverable:** Clean, functional Streamlit app focused on core RAG workflow

---

## Phase 3: Simple Upload Integration  
**Goal:** Implement straightforward document upload and basic validation.

### Core Upload Features
1. **Multi-Source Document Input**:
   - Direct file upload to /data/uploads directory
   - URL-based document download and storage
   - Basic document validation (file type, size)
   - Duplicate detection before processing

2. **Simple Processing Flow**:
   - Immediate processing after upload/download
   - Basic error handling and user feedback
   - Processing status indication

3. **Content Validation Pipeline**:
   ```python
   Upload/URL → Local Storage → Format Validation → Immediate Processing → Vector Store
        ↓           ↓               ↓                     ↓                    ↓
    File/URL    /data/uploads    MIME/Size           Content Extraction      Ready for
    Processing  + SQLite DB      Validation          & Embedding             Queries
                Metadata
   ```

**Deliverable:** Simple, reliable upload system with immediate processing

---

## Phase 4: Advanced Text Extraction & Multi-Modal Processing  
**Goal:** Extract maximum value from diverse document types with state-of-the-art techniques.

### Multi-Modal Extraction Strategy
1. **Layout-Aware PDF Processing**:
   - Use `pymupdf` + `pdfplumber` for precise text extraction
   - Table detection and structured data extraction
   - Figure/diagram extraction with captions

2. **Advanced Chunking Strategies**:
   - **Semantic Chunking**: Use sentence transformers to identify natural breakpoints
   - **Hierarchical Chunking**: Preserve document structure (sections, subsections)
   - **Context-Preserving Overlap**: Smart overlap based on topic coherence

3. **Metadata Enrichment with SQLite**:
   ```python
   # SQLite enables structured metadata with relationships
   chunk_metadata = {
       "document_id": int,  # Foreign key to documents table
       "chunk_type": "text|table|figure|equation",
       "semantic_level": 1-5,  # heading level
       "topic_tags": List[str],  # Stored as JSON in SQLite
       "difficulty_score": float,
       "prerequisite_concepts": List[str],
       "embedding_id": str,  # Reference to vector database
       "parent_chunk_id": Optional[int]  # Hierarchical relationships
   }
   
   # SQLite queries enable powerful filtering:
   # SELECT * FROM chunks WHERE document_id=? AND semantic_level<=2
   # SELECT * FROM documents WHERE processing_status='completed'
   ```

4. **Mathematical Content Processing**:
   - LaTeX equation parsing and indexing
   - Mathematical concept extraction using SymPy
   - Formula-to-text conversion for better searchability

### Advanced Embedding Strategy
- **Multi-Vector Embeddings**: Different models for different content types
- **Contextualized Embeddings**: Include surrounding context in embeddings
- **Hierarchical Embeddings**: Document-level, section-level, and chunk-level vectors

**Deliverable:** Sophisticated extraction pipeline supporting all educational content types

---

## Phase 5: Multi-Route Retrieval System  
**Goal:** Implement sophisticated retrieval that adapts to different query types and learning contexts.

### Query Analysis & Routing
```python
Query → Intent Classification → Route Selection → Specialized Retrieval
   ↓           ↓                    ↓                 ↓
Natural     Factual/            Conceptual/        Dense/Sparse/
Language    Procedural/         Comparative/       Graph-based
Input       Definitional        Exploratory        Retrieval
```

### Specialized Retrieval Routes
1. **Conceptual Learning Route**:
   - Graph-based traversal for prerequisite concepts
   - Progressive difficulty retrieval
   - Cross-reference related topics

2. **Problem-Solving Route**:
   - Example-based retrieval for similar problems
   - Step-by-step solution patterns
   - Method comparison and selection

3. **Research Route**:
   - Citation network traversal
   - Latest findings prioritization
   - Contradictory viewpoint identification

### Advanced Retrieval Endpoints
- `POST /retrieve/adaptive` → context-aware multi-route retrieval
- `POST /retrieve/conceptual` → prerequisite-aware concept retrieval
- `POST /retrieve/comparative` → side-by-side concept comparison
- `POST /retrieve/progressive` → difficulty-graded content retrieval

**Deliverable:** Intelligent retrieval system that adapts to learning objectives

---

## Phase 6: Hybrid Retrieval with Query Enhancement  
**Goal:** Implement state-of-the-art hybrid retrieval with query optimization techniques.

### Query Enhancement Pipeline
```python
Original Query → Query Decomposition → Query Expansion → Multi-Vector Search → Result Fusion
      ↓               ↓                    ↓                ↓                   ↓
   Raw Input    Sub-questions +        Synonyms +        Dense + Sparse      Reciprocal
               Dependencies          Domain Terms      + Graph Search       Rank Fusion
```

### Advanced Query Techniques
1. **Query Decomposition**:
   - Break complex queries into atomic questions
   - Identify dependencies between sub-queries
   - Generate parallel vs. sequential execution plans

2. **Query Translation & Expansion**:
   - Translate colloquial terms to academic terminology
   - Expand with domain-specific synonyms
   - Add implicit context from user's learning history

3. **Multi-Vector Hybrid Search**:
   - **Dense Retrieval**: Multiple embedding models (general + domain-specific)
   - **Sparse Retrieval**: BM25 with domain-specific term weighting
   - **Graph Retrieval**: Knowledge graph traversal for concept relationships

4. **Advanced Fusion Strategies**:
   - **Reciprocal Rank Fusion**: Combine rankings from multiple retrievers
   - **Score Normalization**: Adaptive scoring based on query complexity
   - **Result Diversification**: Ensure diverse perspectives and sources

**Deliverable:** State-of-the-art hybrid retrieval with comprehensive query optimization

---

## Phase 7: Multi-Stage Reranking & Intelligent Compression  
**Goal:** Implement sophisticated reranking with pedagogical relevance scoring and adaptive compression.

### Multi-Stage Reranking Pipeline
```python
Retrieved Results → Pedagogical Scoring → Cross-Encoder Reranking → Context Compression
        ↓                  ↓                      ↓                      ↓
   Top-K Chunks      Educational Value      Semantic Relevance      Optimal Context
   (K=100-200)       Assessment            Re-scoring               (within budget)
```

### Advanced Reranking Features
1. **Pedagogical Relevance Scoring**:
   ```python
   pedagogical_score = (
       concept_clarity * 0.3 +
       example_richness * 0.25 +
       prerequisite_alignment * 0.2 +
       difficulty_appropriateness * 0.15 +
       citation_quality * 0.1
   )
   ```

2. **Multi-Model Ensemble Reranking**:
   - Cross-encoder models fine-tuned on educational content
   - Domain-specific rerankers for different subjects
   - Weighted ensemble based on query characteristics

3. **Adaptive Context Compression**:
   - **Semantic Redundancy Removal**: Eliminate duplicate information
   - **Importance-Based Pruning**: Keep high-impact sentences
   - **Coherence Preservation**: Maintain logical flow in compressed context

4. **Query-Adaptive Token Budgeting**:
   - Dynamic context window allocation based on query complexity
   - Reserve tokens for reasoning chains in complex queries
   - Prioritize primary sources over secondary references

### Compression Strategies
- **Extractive Summarization**: Select most relevant sentences
- **Abstractive Compression**: Generate concise representations
- **Hierarchical Compression**: Multi-level detail based on importance

**Deliverable:** Sophisticated reranking and compression system optimized for educational content

---

## Phase 8: Answer Generation with Educational Focus  
**Goal:** Generate pedagogically optimized responses with comprehensive citations.

### Multi-LLM Generation Strategy
```python
Context + Query → LLM Routing → Specialized Generation → Response Enhancement
      ↓              ↓              ↓                      ↓
  Processed       Model         Domain-Specific         Citations +
   Input         Selection      Response Generation     Visualizations
```

### Specialized Generation Models
1. **Explanation Generator**: Optimized for clear, step-by-step explanations
2. **Example Generator**: Creates relevant examples and analogies
3. **Comparison Generator**: Highlights differences and similarities
4. **Summary Generator**: Provides concise overviews and key takeaways

### Educational Response Features
1. **Adaptive Complexity**:
   - Adjust explanation depth based on user's knowledge level
   - Progressive disclosure for complex topics
   - Multiple explanation strategies (visual, mathematical, analogical)

2. **Interactive Elements**:
   - Embedded quizzes and self-assessment questions
   - Follow-up question suggestions
   - Related concept recommendations

3. **Rich Citations & Sources**:
   ```python
   citation_format = {
       "source_id": str,
       "chunk_id": str,
       "confidence": float,
       "page_number": Optional[int],
       "relevance_score": float,
       "source_type": "textbook|paper|note|slide"
   }
   ```

4. **Multi-Modal Response Generation**:
   - Generate ASCII diagrams for concepts
   - Create structured tables for comparisons
   - Embed mathematical formulas with explanations

### Advanced Prompt Engineering
- **Chain-of-Thought Prompting**: For complex reasoning tasks
- **Tree-of-Thoughts**: For multi-perspective analysis
- **Self-Consistency**: Multiple reasoning paths for verification

**Deliverable:** Advanced generation system producing high-quality educational responses

---

## Phase 9: Streamlit UI Integration  
**Goal:** Create a seamless Streamlit interface for document upload and question answering.

### Core Streamlit Features
1. **Document Upload Section**:
   - `st.file_uploader()` for multiple file types (PDF, DOC, TXT)
   - `st.text_input()` for URL input
   - Upload progress with `st.progress()`

2. **Query Interface**:
   - `st.text_area()` for question input
   - `st.button()` for submission
   - `st.spinner()` during processing

3. **Response Display**:
   - `st.markdown()` for formatted responses
   - `st.expander()` for citation details
   - `st.info()` for source references

4. **Document Management**:
   - `st.dataframe()` for uploaded documents list
   - `st.columns()` for document actions
   - `st.sidebar` for document navigation

**Deliverable:** Fully functional Streamlit app with intuitive RAG interface

---

---

---

## Advanced RAG Enhancements Summary

### Query Enhancement Techniques
1. **Query Decomposition**: Break complex educational queries into atomic components
2. **Query Translation**: Convert colloquial to academic terminology
3. **Query Expansion**: Add synonyms and related concepts from domain knowledge
4. **Intent Classification**: Route queries to specialized retrieval strategies

### Retrieval Innovations
1. **Multi-Vector Dense Retrieval**: Combine general and domain-specific embeddings
2. **Contextualized Sparse Retrieval**: BM25 with educational term weighting
3. **Graph-Based Traversal**: Follow concept relationships and prerequisites
4. **Temporal Awareness**: Prioritize recent research while respecting foundational concepts

### Reranking Excellence
1. **Pedagogical Scoring**: Evaluate educational value and clarity
2. **Cross-Encoder Ensemble**: Multiple models for different query types
3. **Diversity Enforcement**: Ensure multiple perspectives and approaches
4. **Adaptive Thresholding**: Dynamic filtering based on query complexity

---

## Architecture Decision Records

### ADR-1: Microservice vs. Monolithic Architecture
**Decision**: Adopt microservice architecture for scalability and maintainability
**Reasoning**: Enables independent scaling of compute-intensive components (embedding, reranking)

### ADR-2: Database Architecture Selection
**Decision**: Use SQLite for metadata storage with Qdrant for vector embeddings and local filesystem for documents
**Reasoning**: SQLite provides structured metadata management, relationship tracking, and full-text search while maintaining simplicity. Qdrant handles vector similarity search optimally for educational RAG.

### ADR-3: LLM Strategy
**Decision**: Multi-LLM approach with specialized models for different tasks
**Reasoning**: Educational content benefits from task-specific optimization

### ADR-4: Real-time vs. Batch Processing
**Decision**: Hybrid approach - real-time for queries, batch for document processing
**Reasoning**: Balances user experience with computational efficiency

---

## Continuous Improvement Strategy

### Feedback Loops
1. **User Interaction Analysis**: Track query patterns and satisfaction
2. **Educational Outcome Measurement**: Assess learning effectiveness
3. **System Performance Monitoring**: Optimize based on usage patterns
4. **Content Quality Assessment**: Continuously improve document processing

### Model Evolution
1. **Regular Model Updates**: Keep embedding and generation models current
2. **Custom Fine-tuning**: Adapt models to user's specific educational content
3. **A/B Testing**: Experiment with different RAG configurations
4. **Domain Adaptation**: Specialize for different academic subjects

**Key Principles:**
* **Educational First**: Every design decision optimized for learning outcomes
* **Modular Excellence**: Best-in-class components that work seamlessly together  
* **Intelligent Adaptation**: System learns and improves from every interaction
* **Production Ready**: Built for scale, security, and maintainability from day one
* **Measurable Impact**: Comprehensive metrics to validate educational effectiveness